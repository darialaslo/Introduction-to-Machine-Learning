{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading/processing the images  \n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.image import img_to_array \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
    "\n",
    "# models \n",
    "from tensorflow.keras.applications.vgg16 import VGG16 \n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# clustering and dimension reduction\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for everything else\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '/Users/darialaslo/Documents/CBB/CBB SEM2/IML/GIT/task-4/food'\n",
    "os.chdir(path)\n",
    "\n",
    "# this list holds all the image filename\n",
    "food = []\n",
    "\n",
    "#add image files to the list\n",
    "with os.scandir(path) as files:\n",
    "  # loops through each file in the directory\n",
    "    for file in files:\n",
    "        if file.name.endswith('.jpg'):\n",
    "          # adds only the image files to the flowers list\n",
    "            food.append(file.name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# load the image as a 224x224 array\n",
    "img = load_img(food[0], target_size=(224,224))\n",
    "# convert from 'PIL.Image.Image' to numpy array\n",
    "img = np.array(img)\n",
    "\n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "reshaped_img = img.reshape(1,224,224,3)\n",
    "print(reshaped_img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 258s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "\n",
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file, model):\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img) \n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # get the feature vector\n",
    "    features = model.predict(imgx, use_multiprocessing=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN unless changes have been made (load pickle file: dict.pkl)\n",
    "\n",
    "data={}\n",
    "\n",
    "# lop through each image in the dataset\n",
    "for i in range(0,len(food)):\n",
    "\n",
    "    # try to extract the features and update the dictionary\n",
    "    feat = extract_features(food[i],model)\n",
    "    data[food[i]] = feat\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dict as pickle file \n",
    "#features don't have to be extracted again, you can just load this as a pickle file\n",
    "a_file = open(\"dict.pkl\", \"wb\")\n",
    "pickle.dump(data, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 4096)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "feat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4096)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape so that there are 210 samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n",
    "feat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting anchor, pos and neg \n",
    "def get_image_path (file):\n",
    "    \n",
    "    anchor_list=[]\n",
    "    positive_list=[]\n",
    "    negative_list=[]\n",
    "    for i in range (0,file.shape[0]):\n",
    "        anchor_name= \"{}.jpg\".format(file[i,0])\n",
    "        positive_name= \"{}.jpg\".format(file[i,1])\n",
    "        negative_name= \"{}.jpg\".format(file[i,2])\n",
    "        anchor_list.append(anchor_name)\n",
    "        positive_list.append(positive_name)\n",
    "        negative_list.append(negative_name)\n",
    "    \n",
    "    return anchor_list, positive_list, negative_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if using another neural net after extracting fetures, the opposite triplets and labels are needed\n",
    "\n",
    "# use the get image name to know which one is which\n",
    "train_file = open(\"./train_triplets.txt\", \"r\")\n",
    "train = np.loadtxt(train_file, dtype=str)\n",
    "\n",
    "anchor_list1, positive_list1, negative_list1 = get_image_path (train)\n",
    "\n",
    "#need to append the opposite order anchor, negative positive, that will have label 0\n",
    "anchor_list2, nagtive_list2, positive_list2 = get_image_path (train)\n",
    "#append to the previous\n",
    "anchor_list1.append(anchor_list2)\n",
    "positive_list1.append(positive_list2)\n",
    "negative_list1.append(negative_list2)\n",
    "\n",
    "#create the dataset by taking the feature vectors for each on the tree images in each triplet and adding it \n",
    "#for the train set it should be twice the size\n",
    "#doing this in a function would be helpful so that we can use it for test \n",
    "\n",
    "\n",
    "#create labels for the train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from previous model\n",
    "#dimensions/shapes need to be adjusted if it is to be used \n",
    "\n",
    "#define parameters\n",
    "aminoacids=4\n",
    "features=20\n",
    "fnode_1=16\n",
    "fnode_2=12\n",
    "fnode_3=10\n",
    "\n",
    "#cin = Input(batch_shape=(None,2))\n",
    "pin = Input(batch_shape=(None, aminoacids, features))\n",
    "\n",
    "#cout = cin\n",
    "#pout = pin\n",
    "\n",
    "\n",
    "layer_1 = Dense(fnode_1, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2 (1.0e-4))(pin)\n",
    "#out = pout\n",
    "\n",
    "#out = concatenate([pout, cout])\n",
    "layer_2 = Dense(fnode_2, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2 (1.0e-4))(layer_1)\n",
    "layer_3 = Dense(fnode_3, kernel_initializer='he_uniform', activation='relu', kernel_regularizer=l2 (1.0e-4))(layer_2)\n",
    "\n",
    "\n",
    "out = Dropout(0.5)(layer_3)\n",
    "out = Dense(1, kernel_initializer='he_uniform', activation='sigmoid')(out)\n",
    "\n",
    "#adam=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "    \n",
    "model = Model(inputs=[pin], outputs=[out])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "#summarise the model\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
